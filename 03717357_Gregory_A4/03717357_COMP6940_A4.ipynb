{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c265dc6ea9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNMF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wordcloud\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from .wordcloud import (WordCloud, STOPWORDS, random_color_func,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         get_single_color_func)\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcolor_from_image\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m __all__ = ['WordCloud', 'STOPWORDS', 'random_color_func',\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PILLOW_VERSION'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         raise ImportError(\"The _imaging extension was built for another \"\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.utils import shuffle\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Apple Orange Orange Apple','Apple Banana Apple Banana','Banana Apple Banana Banana Banana Apple',\\\n",
    "          'Banana Orange Banana Banana Orange Banana','Banana Apple Banana Banana Orange Banana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a function to vectorize the counts of a list of sentences. Does not ignore case.'''\n",
    "def myCountVec(corpus):\n",
    "    bag = set()  # use a set to figure out how many unique words there are\n",
    "    word_index = dict() # use a dict to store the column index of the word\n",
    "    for line in corpus:\n",
    "        for word in line.split():\n",
    "            if word in bag:\n",
    "                continue\n",
    "            else:\n",
    "                bag.add(word)\n",
    "    list_of_words = sorted(list(bag)) # sort the list of words derived from set\n",
    "    for index,word in enumerate(list_of_words):\n",
    "        word_index[word] = index  # assign index to word\n",
    "    vec_of_counts = np.zeros((len(corpus),len(word_index))) # create dummy matrix with all locations initilised to 0\n",
    "    for row,line in enumerate(corpus):\n",
    "        for word in line.split():\n",
    "            col = word_index[word]\n",
    "            vec_of_counts[row][col] += 1  # populate the matrix, increasing a word count when needed.\n",
    "    return vec_of_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCountVec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('MrTrumpSpeeches.csv', sep='~', encoding='latin1')\n",
    "df['sentiment'] = np.where(df['like_count'] > df['dislike_count'], 1, 0)\n",
    "df = shuffle(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check some characteristics of the datframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok so 836 rows by 10 columns. Lets check the numeric columns for issues\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok so far so good. Lets check the non numeric columns\n",
    "for col in ['id','playlist','title','subtitles']:\n",
    "    print(df[col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so looks like we have all the data. Lets check for missing data and nans\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so there are some genral punctuation marks that should be removed since they dont add anything to the context.\n",
    "# Some word are enclosed inside [] indicating environment and not really contributing to the sentiment anaylysis\n",
    "def cleaningFunc(line):\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    line = re.sub('[\\s]{2,}',' ',line)\n",
    "    line = re.sub('[\\/\\+\\-,:.\\'\\$*%\\&]','',line) # remove puctuation marks etc.\n",
    "    line = line.lower() # convert everthing to lowercase\n",
    "    words = line.split() # split into words\n",
    "    newwords = []\n",
    "    for word in words:\n",
    "        if word.startswith('[') or word.endswith(']'): # skip words enclosed in brackets since they indicate action not sentiment or topic\n",
    "            continue\n",
    "        word = word.strip(' ') # remove any trailing or leading spaces\n",
    "        newwords.append(word)\n",
    "    return \" \".join(newwords) # return sentence with word seperated by a single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subtitle_clean'] = df['subtitles'].apply(lambda x : cleaningFunc(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification tests I will be using F1 score because we are doing binary classification and this score incorporates\n",
    "both false positives and false negatives in the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choosing features that are >5% and <95% frequent\n",
    "cvecs = CountVectorizer(max_df=0.95,min_df=0.05,stop_words='english') \n",
    "Xcv = cvecs.fit_transform(df['subtitle_clean'].values)\n",
    "print(\"Count matrix shape :\", Xcv.shape)\n",
    "tfvecs = TfidfVectorizer(max_df=0.95,min_df=0.05,stop_words='english') \n",
    "Xtf = tfvecs.fit_transform(df['subtitle_clean'].values)\n",
    "print(\"Tfidf matrix shape :\", Xtf.shape)\n",
    "# lets makes the 1 to 3 word ngram tfidf feature matrix\n",
    "tfvecsngram = TfidfVectorizer(max_df=0.95,min_df=0.05,stop_words='english',ngram_range=(1,3))\n",
    "Xtfng = tfvecsngram.fit_transform(df['subtitle_clean'].values)\n",
    "print(\"Tfidf ngram matrix shape :\", Xtfng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run a loop for all the tests and store the f1 score in a list for plotting later\n",
    "tests = {1:'Logistic regression model on word count',2:'Logistic regression model on TFIDF',\\\n",
    "         3:'Logistic regression model on TFIDF + ngram',4:'Support Vector Machine model on word count',\\\n",
    "         5:'Support Vector Machine model on TFIDF',6:'Support Vector Machine model on TFIDF + ngram'}\n",
    "results = []\n",
    "for index,dataset in enumerate([Xcv,Xtf,Xtfng]*2):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(dataset,df['sentiment'],test_size=0.2,random_state=0)\n",
    "    if index < 3:\n",
    "        logistic = LogisticRegression()\n",
    "        logistic.fit(X_train,y_train)\n",
    "        y_pred = logistic.predict(X_test)\n",
    "        score = int(f1_score(y_test,y_pred) * 10000) / 100.0\n",
    "        print(\"F1 score for test {} is {}%\".format(tests[index+1],score))\n",
    "        results.append((index+1,score))\n",
    "    elif index < 6:\n",
    "        support_vector = LinearSVC()\n",
    "        support_vector.fit(X_train,y_train)\n",
    "        y_pred = support_vector.predict(X_test)\n",
    "        score = int(f1_score(y_test,y_pred) * 10000) / 100.0\n",
    "        print(\"F1 score for test {} is {}%\".format(tests[index+1],score))\n",
    "        results.append((index+1,score))\n",
    "    else:\n",
    "        pass\n",
    "print(\"Tests complete\")\n",
    "###########################################################################################\n",
    "# lets plot the barplot of values\n",
    "plotting_df = pd.DataFrame(list(tests.items()),columns=['index','Test'])\n",
    "plotting_df['score'] = [x[1] for x in results]\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "sns.barplot(x='index',y='score',hue='Test',data=plotting_df,ax=ax)\n",
    "plt.xticks([])\n",
    "plt.xlabel('') # remove xlabels\n",
    "plt.ylabel('Percentage accuracy',fontsize=15)\n",
    "plt.title(\"F1 scores for Logistic Regression vs LinearSVC\",fontsize=15)\n",
    "ax.legend(loc=5)\n",
    "for bar in ax.patches:\n",
    "    x = bar.get_x()\n",
    "    width = bar.get_width()\n",
    "    centre = x + width/2.\n",
    "    bar.set_x(centre - 0.5/2.)\n",
    "    bar.set_width(0.5)\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(\"%.2f\" % height, (x + width / 2., height),\n",
    "             ha='center', va='center', rotation=0, xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling (20 marks)\n",
    "1. Using TFIDF and Count Vectorizer models imported for sklearn, perform topic modelling using the following topic modeling algorithms:\n",
    "    1. NMF\n",
    "    2. LDA\n",
    "    3. SVD\n",
    "\n",
    "2. When choosing the number of topics give a brief explanation of why that number was chosen.\n",
    "3. Discuss based on the top 10 words each of the algorithms choose for each topic cluster what category the topics fall under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(df['subtitle_clean'])[:100] # choose sample of documents\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english',ngram_range=(1,5))\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "n_topics = 10\n",
    "no_top_words = 10\n",
    "no_top_documents = 4\n",
    "\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "#         print(\"\") \n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf_H):\n",
    "    print(\"Topic {}:\".format(topic_idx))\n",
    "    print(\" \".join([tfidf_feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_\n",
    "display_topics(lda_H, lda_W,tf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = TruncatedSVD(n_components=n_topics, n_iter=12, random_state=1).fit(tfidf)\n",
    "lsi_W = lsi_model.transform(tfidf)\n",
    "lsi_H = lsi_model.components_\n",
    "display_topics(lsi_H, lsi_W, tfidf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (10 marks)\n",
    "Choose the clusters obtained from a topic model algorithm from above and plot a word cloud\n",
    "1. for each of the clusters. For example, if the number of topics chosen was 10 and the topics were obtained from the SVD algorithm, 10 word clouds should be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc = {} # lets make a dict to hold the top docs\n",
    "for topic_idx, topic in enumerate(lsi_H):\n",
    "    top_doc_indices = np.argsort( lsi_W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "    tmp = ''\n",
    "    for doc_index in top_doc_indices:\n",
    "        tmp += df['subtitle_clean'][doc_index]\n",
    "    full_doc[topic_idx] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make some wordcluds to visualize the results\n",
    "plt.rcParams['font.size']=12                #10 \n",
    "plt.rcParams['savefig.dpi']=100             #72 \n",
    "plt.rcParams['figure.subplot.bottom']=.1 \n",
    "f, axes = plt.subplots(len(full_doc),1,figsize=(25,25))\n",
    "for topic in full_doc:\n",
    "    text = full_doc[topic]\n",
    "    wc = WordCloud(width=1000,height=500,background_color='white',max_words=20,\\\n",
    "                  random_state=1).generate(text)\n",
    "    axes[topic].imshow(wc)\n",
    "    axes[topic].set_title(\"Topic number = {}\".format(topic))\n",
    "    axes[topic].set_xticks([])\n",
    "    axes[topic].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"a     \\n bbbbbb      c\"\n",
    "s = re.sub('[\\s]{2,}',' ',s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['abc','ab','abcd','acd']\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
