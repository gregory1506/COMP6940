{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03717357 Gregory Ollivierre COMP6040 Assignment2 Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[2] pyspark-shell\"\n",
    "os.environ['JAVA_HOME'] = \"/home/gregory/jdk1.8.0_161/\"\n",
    "import findspark\n",
    "spark_dir = '/home/gregory/spark-2.3.0-bin-hadoop2.7/'\n",
    "findspark.init(spark_dir)\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 3),\n",
       " ('u', 2),\n",
       " ('w', 2),\n",
       " ('j', 1),\n",
       " ('b', 1),\n",
       " ('g', 1),\n",
       " ('n', 2),\n",
       " ('c', 1),\n",
       " ('d', 2),\n",
       " ('o', 5),\n",
       " ('y', 3),\n",
       " ('v', 2),\n",
       " ('s', 2),\n",
       " ('m', 1),\n",
       " ('t', 2),\n",
       " ('l', 1),\n",
       " ('k', 1),\n",
       " ('h', 4),\n",
       " ('p', 3),\n",
       " ('T', 1),\n",
       " ('x', 3),\n",
       " ('f', 2),\n",
       " ('q', 1),\n",
       " ('a', 4),\n",
       " ('e', 5),\n",
       " ('i', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the following sentence into a python tuple list of letters and the frequency of which each letter appears in the current word.\n",
    "#Ignore all non-alpha numeric characters.\n",
    "sentence = \"The quick brown fox jumps over the laxy dog and the fox was very happy\"\n",
    "# I treated 'T' and 't' as two seperate chars since they are different. Alternatively sentence.tolower()\n",
    "# could be used\n",
    "samp_tup_list = [(x,sentence.count(x)) for x in set(sentence.replace(\" \",''))]\n",
    "samp_tup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyspark contect\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of tuples to rdd\n",
    "rdd = sc.parallelize(samp_tup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 3),\n",
       " ('u', 2),\n",
       " ('w', 2),\n",
       " ('j', 1),\n",
       " ('b', 1),\n",
       " ('g', 1),\n",
       " ('n', 2),\n",
       " ('c', 1),\n",
       " ('d', 2),\n",
       " ('o', 5),\n",
       " ('y', 3),\n",
       " ('v', 2),\n",
       " ('s', 2),\n",
       " ('m', 1),\n",
       " ('t', 2),\n",
       " ('l', 1),\n",
       " ('k', 1),\n",
       " ('h', 4),\n",
       " ('p', 3),\n",
       " ('T', 1),\n",
       " ('x', 3),\n",
       " ('f', 2),\n",
       " ('q', 1),\n",
       " ('a', 4),\n",
       " ('e', 5),\n",
       " ('i', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display letter count\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', [1, 1, 1]),\n",
       " ('u', [1, 1]),\n",
       " ('w', [1, 1]),\n",
       " ('j', [1]),\n",
       " ('b', [1]),\n",
       " ('g', [1]),\n",
       " ('n', [1, 1]),\n",
       " ('c', [1]),\n",
       " ('d', [1, 1]),\n",
       " ('o', [1, 1, 1, 1, 1]),\n",
       " ('y', [1, 1, 1]),\n",
       " ('v', [1, 1]),\n",
       " ('s', [1, 1]),\n",
       " ('m', [1]),\n",
       " ('t', [1, 1]),\n",
       " ('l', [1]),\n",
       " ('k', [1]),\n",
       " ('h', [1, 1, 1, 1]),\n",
       " ('p', [1, 2]),\n",
       " ('T', [1]),\n",
       " ('x', [1, 1, 1]),\n",
       " ('f', [1, 1]),\n",
       " ('q', [1]),\n",
       " ('a', [1, 1, 1, 1]),\n",
       " ('e', [1, 1, 1, 1, 1]),\n",
       " ('i', [1])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the letter and number of times they appear in each word in the sentence\n",
    "rdd.map(lambda x : (x[0],[y.count(x[0]) for y in sentence.split() if x[0] in y])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new pyspark contect\n",
    "spark2 = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "sc2 = spark2.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',id,name,username',\n",
       " '0,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Cristina M',\n",
       " '1,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Ricky',\n",
       " '2,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Tedd Gardiner',\n",
       " '3,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Dougal']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets open the csv and save to rdd\n",
    "rdd2 = sc2.textFile('amazon_reviews.csv')\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Cristina M'],\n",
       " ['1', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Ricky'],\n",
       " ['2', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Tedd Gardiner'],\n",
       " ['3', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Dougal'],\n",
       " ['4', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Miljan David Tanic']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looks like comma seperated data. Lets split by ',' and remove first line since we add schema later\n",
    "rdd3 = rdd2.map(lambda line: line.split(','))\n",
    "rdd3 = rdd3.filter(lambda line : line[0] != '')\n",
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+\n",
      "|                 idn|          product|            username|\n",
      "+--------------------+-----------------+--------------------+\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|          Cristina M|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|               Ricky|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|       Tedd Gardiner|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|              Dougal|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|  Miljan David Tanic|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|          Kelvin Law|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|               Ricky|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|             Bandler|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|          Cristina M|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|       Tedd Gardiner|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|     Miguel Martinez|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|   Magnus Brattemark|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|       Tedd Gardiner|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|  Janet Matthews Jan|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|John Kat's the br...|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|              samira|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|        Louis simard|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|              JanetC|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|            Shepherd|\n",
      "|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|              Brenda|\n",
      "+--------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe with schema (index,idn,product,username)\n",
    "df = rdd3.map(lambda line: Row(index=line[0],idn=line[1],product=line[2],username=line[3])).toDF()\n",
    "df = df.drop('index') # drop index since its prob nor required\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             product|count|\n",
      "+--------------------+-----+\n",
      "|Amazon Tap - Alex...|  542|\n",
      "|      Amazon Fire TV|  166|\n",
      "|Amazon Premium He...|  133|\n",
      "|    Fire HD 6 Tablet|   87|\n",
      "|\"Kindle Fire HDX ...|   53|\n",
      "|\"Kindle Fire HDX ...|   43|\n",
      "|\"Kindle Fire HD 7\"\"\"|   41|\n",
      "|   Kindle Paperwhite|   39|\n",
      "|Certified Refurbi...|   38|\n",
      "|     Kindle Keyboard|   32|\n",
      "|All-New Amazon Fi...|   27|\n",
      "|              Kindle|   20|\n",
      "|Amazon 5W USB Off...|   19|\n",
      "|All-New Amazon Fi...|   18|\n",
      "|Replacement Remot...|   17|\n",
      "|Echo Dot (2nd Gen...|   13|\n",
      "|All-New Amazon Ki...|   13|\n",
      "|Moshi Anti-Glare ...|   12|\n",
      "|All-New Amazon Ki...|   12|\n",
      "|Alexa Voice Remot...|   12|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display top 20 bought products\n",
    "df.groupby('product').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "|           username|             product|count|\n",
      "+-------------------+--------------------+-----+\n",
      "|          A. Younan|Amazon Premium He...|   59|\n",
      "|             Andrew|Amazon Premium He...|   43|\n",
      "|          Victor L.|Amazon Premium He...|   30|\n",
      "|     William Hardin|    Fire HD 6 Tablet|   30|\n",
      "|            Mike W.|    Fire HD 6 Tablet|   29|\n",
      "|      Earthling1984|    Fire HD 6 Tablet|   28|\n",
      "|     William Hardin|      Amazon Fire TV|   16|\n",
      "|         B. Tarbuck|\"Kindle Fire HDX ...|   16|\n",
      "|              Mandy|      Amazon Fire TV|   16|\n",
      "|                 NF|\"Kindle Fire HDX ...|   15|\n",
      "|                 NF|\"Kindle Fire HD 7\"\"\"|   14|\n",
      "|    Amazon Reviewer|\"Kindle Fire HDX ...|   14|\n",
      "|    Amazon Reviewer|\"Kindle Fire HDX ...|   13|\n",
      "|             Dallas|      Amazon Fire TV|   12|\n",
      "|     William Hardin|Certified Refurbi...|   12|\n",
      "|  Michael Gallagher|\"Kindle Fire HDX ...|   12|\n",
      "|   Gregory P. Baker|      Amazon Fire TV|   11|\n",
      "|              MarkM|\"Kindle Fire HD 7\"\"\"|   10|\n",
      "|              Mandy|Certified Refurbi...|   10|\n",
      "|\"Things I Love Like|\"Kindle Fire HDX ...|    9|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display top 20 users and what they bought\n",
    "df.groupby('username','product').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kindle Paperwhite', 'Cristina M'),\n",
       " ('Kindle Paperwhite', 'Ricky'),\n",
       " ('Kindle Paperwhite', 'Tedd Gardiner'),\n",
       " ('Kindle Paperwhite', 'Dougal'),\n",
       " ('Kindle Paperwhite', 'Miljan David Tanic'),\n",
       " ('Kindle Paperwhite', 'Kelvin Law'),\n",
       " ('Kindle Paperwhite', 'Ricky'),\n",
       " ('Kindle Paperwhite', 'Bandler'),\n",
       " ('Kindle Paperwhite', 'Cristina M'),\n",
       " ('Kindle Paperwhite', 'Tedd Gardiner')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a RDD of tuples from the dataframe from question 3 with only 2 columns ‘product’ and ‘username’ in that order.\n",
    "rdd4 = df.select('product','username').rdd.map(tuple)\n",
    "rdd4.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
